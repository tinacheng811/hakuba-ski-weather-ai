import streamlit as st
import pandas as pd
import numpy as np
from tensorflow.keras.models import load_model
import pickle # ç”¨ä¾†è¼‰å…¥ä½ çš„ scaler

# 1. ç¶²é æ¨™é¡Œèˆ‡é¸å–®
st.set_page_config(page_title="ç™½é¦¬æ‘æ»‘é›ªå¤©æ°£AIåŠ©ç†", page_icon="â„ï¸")
st.title("â„ï¸ ç™½é¦¬æ‘æ»‘é›ªå¤©æ°£AIåŠ©ç†")
st.write("é€éLSTMæ·±åº¦å­¸ç¿’æ¨¡å‹é æ¸¬æœ€ä½³æ»‘é›ªæ™‚æ©Ÿ")

# 2. å´é‚Šæ¬„ï¼šä½¿ç”¨è€…è¼¸å…¥
st.sidebar.header("è«‹é¸æ“‡æ‚¨çš„æ—…éŠæœŸé–“:")
start_date = st.sidebar.date_input("é–‹å§‹æ—¥æœŸ:")
end_date = st.sidebar.date_input("çµæŸæ—¥æœŸ:")

# 3. è¼‰å…¥æ¨¡å‹èˆ‡å·¥å…· (é€™éƒ¨åˆ†å»ºè­°å…ˆåœ¨ Colab å„²å­˜å¥½)
# @st.cache_resource ç¢ºä¿æ¨¡å‹åªæœƒè¼‰å…¥ä¸€æ¬¡ï¼Œç¯€çœæ‰‹æ©Ÿé–‹å•Ÿæ™‚é–“
@st.cache_resource
def load_ai_assets():
    model = load_model('my_lstm_model.h5')
    with open('scaler.pkl', 'rb') as f:
        scaler = pickle.load(f)
    return model, scaler

# 4. æ ¸å¿ƒé‚è¼¯ (å°‡ä½ ä¹‹å‰çš„ get_ski_recommendation æ”¾é€²ä¾†)
def get_ski_recommendation(start_date_str, end_date_str, model, scaler, df, window_size=7):
    # 1. æ™‚é–“æ ¼å¼è½‰æ›
    start_date = pd.to_datetime(start_date_str)
    end_date = pd.to_datetime(end_date_str)
    last_date = df['Date'].max()
    
    # 2. å®šç¾©ç‰¹å¾µæ¬„ä½ (å¿…é ˆæ˜¯è¨“ç·´æ™‚çš„ 9 å€‹)
    features_cols = ['tavg', 'tmax', 'tmin', 'prcp', 'snowf', 'snowdmax', 'sunhour', 'month_sin', 'month_cos']
    
    # 3. æº–å‚™å„²å­˜çµæœçš„æ¸…å–®
    predictions_list = []

    # 4. åˆ¤æ–·å€é–“è½åœ¨ã€Œæ­·å²ã€é‚„æ˜¯ã€Œæœªä¾†ã€
    # å¦‚æœå€é–“å®Œå…¨åœ¨æ­·å²å…§ï¼Œç›´æ¥å¾ df æŠ“
    if end_date <= last_date:
        relevant_df = df[(df['Date'] >= start_date) & (df['Date'] <= end_date)]
        for _, row in relevant_df.iterrows():
            predictions_list.append({
                'date': row['Date'],
                'tavg': row['tavg'], 'tmax': row['tmax'], 'tmin': row['tmin'],
                'snowf': row['snowf'], 'snowdmax': row['snowf'] # æ­·å²è³‡æ–™å¯èƒ½å«ä¸åŒå
            })
    else:
        # å¦‚æœåŒ…å«æœªä¾†ï¼ŒåŸ·è¡Œ LSTM éè¿´é æ¸¬
        days_to_predict = (end_date - last_date).days
        
        # æº–å‚™æœ€å¾Œ window_size å¤©çš„è³‡æ–™ä½œç‚ºå•Ÿå‹•ç¨®å­
        last_data_raw = df[features_cols].tail(window_size).fillna(0).values
        current_batch = scaler.transform(last_data_raw).reshape(1, window_size, 9)

        for i in range(days_to_predict):
            # æ¨¡å‹è¼¸å‡º 5 å€‹å€¼: [tavg, tmax, tmin, snowf, snowdmax]
            pred = model.predict(current_batch, verbose=0)[0]
            curr_date = last_date + pd.Timedelta(days=i+1)

            # å¦‚æœé€™å¤©åœ¨éŠå®¢è¦æ±‚çš„å€é–“å…§ï¼Œå‰‡é‚„åŸä¸¦è¨˜éŒ„
            if start_date <= curr_date <= end_date:
                dummy = np.zeros((1, 9))
                dummy[0, 0:3] = pred[0:3] # tavg, tmax, tmin
                dummy[0, 4:6] = pred[3:5] # snowf, snowdmax
                res = scaler.inverse_transform(dummy)[0]
                
                predictions_list.append({
                    'date': curr_date,
                    'tavg': res[0], 'tmax': res[1], 'tmin': res[2],
                    'snowf': res[4], 'snowdmax': res[5]
                })

            # æ›´æ–°ä¸‹ä¸€å¤©çš„è¼¸å…¥ (ç¶­æŒ 9 ç‰¹å¾µ)
            m_sin = np.sin(2 * np.pi * curr_date.month / 12)
            m_cos = np.cos(2 * np.pi * curr_date.month / 12)
            # æ§‹é€  [tavg, tmax, tmin, prcp(0), snowf, snowdmax, sunhour(0.5), m_sin, m_cos]
            new_entry = np.array([pred[0], pred[1], pred[2], 0, pred[3], pred[4], 0.5, m_sin, m_cos]).reshape(1, 1, 9)
            current_batch = np.append(current_batch[:, 1:, :], new_entry, axis=1)

    # 5. è©•åˆ†èˆ‡æ˜Ÿæ˜Ÿè½‰åŒ–é‚è¼¯ (å»¶ç”¨ä¹‹å‰çš„é‚è¼¯)
# 5. è©•åˆ†èˆ‡æ˜Ÿæ˜Ÿè½‰åŒ–é‚è¼¯
    final_scores = []
    for day in predictions_list:
        score = 0
        score += day['snowdmax'] * 1.0  # ç©é›ªæ·±åº¦åˆ†
        if day['snowf'] > 2 and day['tmax'] < 0: 
            score += 30 # ç²‰é›ªåˆ†
        if day['tmax'] > 3: 
            score -= 20 # èé›ªæ‰£åˆ†

        # é€™è£¡è¦ç¢ºä¿æ‰€æœ‰çš„ç¸®é€² (Indentation) éƒ½å°é½Š
        star_count = 1
        if score >= 80: star_count = 5
        elif score >= 50: star_count = 4
        elif score >= 20: star_count = 3
        elif score > 0: star_count = 2

        # --- é—œéµä¿®æ­£å€å¡Š ---
        final_scores.append({
            'date': day['date'],
            'score': score,
            'info': day,
            'stars': "â­" * star_count + "â˜†" * (5 - star_count),
            'tips': get_travel_tips(star_count, day) # æ³¨æ„é€™è¡Œè¦è·Ÿä¸Šé¢çš„å°é½Š
        })
    # ------------------

    if not final_scores:
        return None, []

    best_day = max(final_scores, key=lambda x: x['score'])
    return best_day, final_scores



# ------------------------------------------------------
if st.sidebar.button("é–‹å§‹AIé æ¸¬"):
    with st.spinner('AIæ­£åœ¨è¨ˆç®—é›ªæ³ä¸­...'):
        # é€™è£¡å‘¼å«ä½ ä¹‹å‰å¯«å¥½çš„å‡½å¼
        best_day, all_results = get_ski_recommendation(str(start_date), str(end_date), model, scaler, df)
        
        # é¡¯ç¤ºæœ€ä½³æ—¥æœŸ (æ˜Ÿæ˜Ÿèˆ‡å°æ’‡æ­¥)
        st.success(f"ğŸ† æœ€ä½³æ¨è–¦æ—¥ï¼š{best_day['date'].date()}")
        st.metric("æ¨è–¦æŒ‡æ•¸", best_day['stars'])
        st.info(f"ğŸ’¡ æ•™ç·´å»ºè­°ï¼š{best_day['tips']}")
        
        # é¡¯ç¤ºè©³ç´°æ•¸æ“šè¡¨
        st.subheader("ğŸ“Š è©³ç´°é å ±æ•¸æ“š")
        res_df = pd.DataFrame([r['info'] for r in all_results])
        st.dataframe(res_df)